{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('D:/Nisum/Employees.csv')\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "key_field--> Primary Key filed from dataframe\n",
    "date_filed--> Date file from dataframe\n",
    "current_dateFormat--> format of the existing date_filed which is coming from dataframe\n",
    "custom_dateFromat--> format of date to change\n",
    "\"\"\"\n",
    "\n",
    "def customDateFormat(data_frame, key_field, date_field, current_dateFormat, custom_dateFormat):\n",
    "    \n",
    "    \"\"\"converting columns into list\n",
    "    here we are converting the dataframe values to list values for \"key_field\" and \"date_field\" \n",
    "    using rdd flat map function.those list values will be captured by Key_list and date_list.\"\"\"\n",
    " \n",
    "    key_list = data_frame.select(key_field).rdd.flatMap(lambda x: x).collect()\n",
    "    date_list = data_frame.select(date_field).rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    \"\"\"converting current date fromat into custom date format\n",
    "    Here we are iterating the all date_list valuesto strptime function and changing the \n",
    "    current date format to custom date format using strftime function. converted list of value will be assiged to \"dates\"\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = list(map(lambda x: datetime.strptime(x, current_dateFormat).strftime(custom_dateFormat), date_list))\n",
    "    \n",
    "    \"\"\"Changing list  to dataframe\n",
    "    Now we are converting both list values(\"key_list\", \"dates\") to new datafarme using structtypes. dataframe name \"key_date_df\"\n",
    "    \"\"\"\n",
    "    myschema= StructType([ StructField(key_field, StringType(), True),StructField(date_field, StringType(), True)])\n",
    "    key_date_df=sqlContext.createDataFrame(zip(key_list,dates),schema = myschema)\n",
    "    \n",
    "    \"\"\"\n",
    "    Joining the 2 dataframes(\"key_date_df\",\"data_frame\")  using \"key_field\" and select required fields from both dataframes.\n",
    "    \"\"\"\n",
    "    return data_frame.join(key_date_df, data_frame[key_field] == key_date_df[key_field],how='full').drop(data_frame[date_field]).drop(key_date_df[key_field]).select('*').sort(key_field).dropDuplicates()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = customDateFormat(df, \"EMPLOYEE_ID\", \"HIRE_DATE\", \"%d-%m-%y\", \"%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
